\documentclass{article}

\usepackage{amsmath}
\usepackage{hyperref}

\begin{document}

\title{How \textsc{Horst} Works}
\date{}

\maketitle

\tableofcontents

\newpage

\section{Introduction}
This document describes how the reconstruction of original spectra can be done and how it is implemented in \textsc{Horst}.

\section{Detector Response}

\subsection{Continuous spectrum}
For illustraction purposes, first treat the particle spectrum $s(E)$ as a continuous function of the energy $E$. 
The detected spectrum will be denoted as $\tilde{s}(E)$.

\noindent The content of the detected spectrum at an energy $E$ is related to the original spectrum by a continuous superposition with the detector response function $r(E, E')$:

\begin{equation}
\label{convolution_continuous}
	\tilde{s}(E) = \int r(E, E') \cdot s(E') \mathrm{d}E' 
\end{equation}

\noindent This continuous superposition is often also referred to as the convolution with the detector response due to the similarity of both operations, although the mathematical definition of the convolution differs.
The two arguments of $r$ mean that, in principle, any energy of the detected spectrum can be influenced by any energy of the original spectrum.
The diagonal elements $r(E,E)$ are commonly referred to as the detection efficiency, or the full-energy peak (FEP) efficiency.
For a detector with negligible pileup, 

\begin{equation}
	\label{response_condition}
	r(E, E') = 0 ~\mathrm{if}~ E > E'
\end{equation}

\noindent i.e. a detector will, at maximum, detect the full energy of a particle, and not more. This is also assumed in the code.

\subsection{Discrete Spectrum (Histogram)}
In reality, one almost never measures continuous spectra, but they will be binned in some way.
Denoting all quantities with square brackets instead of parentheses, and variables with an index in order to indicate discrete values, Eq. (\ref{convolution_continuous}) becomes:

\begin{equation}
	\label{convolution_discrete}
	\tilde{s}[E_i] = \sum_{j = 0}^{N} r[E_i][E_j] \cdot s[E_j]
\end{equation}

\noindent The convolution of the response function on the spectrum has now become the action of an $N \times N$ matrix on a vector with $N$ entries:

\begin{align}
	\label{matrix_equation_general}
	\tilde{s} &= r \cdot s = \\
	\label{matrix_equation_explicit}
	\left[ 
		\begin{array}{c}
			\tilde{s}[0] \\
			\tilde{s}[1] \\
			\vdots	\\
			\tilde{s}[N] \\
		\end{array}
	\right]
	&= 
	\begin{bmatrix}
		r[0][0] & r[0][1] & \hdots & r[0][N] \\
		r[1][0] & r[1][1] & \hdots & r[1][N] \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		r[N][0] & r[N][1] & \hdots & r[N][N] \\
	\end{bmatrix}
	\cdot
	\left[ 
		\begin{array}{c}
			s[0] \\
			s[1] \\
			\vdots	\\
			s[N] \\
		\end{array}
	\right]
\end{align}

\noindent Each column $r[*][i]$ of the the matrix is multiplied by a single entry of the original spectrum. This means that it represents the response of the detector to monoenergetic particles with the energy $E_i$.
Each row $r[i][*]$ shows the magnitude of the correlation of this bin with all others.

\subsection{Reconstructing the original Spectrum}

The idealized problem in Eq. (\ref{matrix_equation_general}) can in principle be solved by inversion of the matrix $r$:

\begin{equation}
	\label{matrix_equation_solved}
	r^{-1} \cdot \tilde{s} = s
\end{equation}

\noindent The straightforward solution by inversion can be greatly simplified by applying Eq. (\ref{response_condition}) (no pileup) in discretized form. 
Then, the matrix in Eq. (\ref{matrix_equation_general}) becomes an upper triangular matrix:

\begin{equation}
	\label{matrix_equation_triangle_explicit}
	\left[ 
		\begin{array}{c}
			\tilde{s}[0] \\
			\tilde{s}[1] \\
			\vdots	\\
			\tilde{s}[N] \\
		\end{array}
	\right]
	= 
	\begin{bmatrix}
		r[0][0] & r[0][1] & \hdots & r[0][N] \\
		0       & r[1][1] & \hdots & r[1][N] \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & \hdots & r[N][N] \\
	\end{bmatrix}
	\cdot
	\left[ 
		\begin{array}{c}
			s[0] \\
			s[1] \\
			\vdots	\\
			s[N] \\
		\end{array}
	\right]
\end{equation}

\noindent This problem is much easier to solve than with a completely general matrix.
The solution algorithm is based on Gaussian elimination and sometimes called the TopDown algorithm in this context.
It can be visualized as subsequently subtracting the correctly scaled response function of the highest energy bin from the detected spectrum to "peel away" the response and leave only the original spectrum.

\noindent Using the expression for the solution of a triangular linear equation, the $i$-th bin content of the original spectrum is given by:

\begin{equation}
	\label{topdown_solution}
	s[i] = \frac{\tilde{s}[i] - \sum_{j = i+1}^{N} r[i][j] s[j]}{r[i][i]}
\end{equation}

\noindent Please note the recursive nature of Eq.~\eqref{topdown_solution}.
This algorithm, like all direct solutions of Eq. (\ref{matrix_equation_general}), has the drawback that it can yield unphysical results, i.e negative bin contents in the original spectrum $s$, due to imperfect response simulations or energy calibrations, or -most importantly- statistical effects:


\noindent In a real detection experiment, there is the impact of counting statistics, which causes the bin contents of $\tilde{s}$ to deviate from the large-number limit by a factor $\epsilon$:

\begin{equation}
	\label{matrix_equation_statistics}
	\tilde{s} + \epsilon \approx r \cdot s
\end{equation}

\noindent With statistical fluctuations present, the equality of the two parts of Eq.~\eqref{matrix_equation_general} does not hold any more.

\noindent While imperfect calibrations and simulation can in principle be improved, statistical fluctuations are always present, at least in the observed spectrum and probably also in the response matrix (see Sec.~\ref{sec:response_simulations}).

\noindent Negative bin contents can be avoided by imposing the condition

\begin{equation}
	\label{no_zeros}
	s[i] \geq 0 ~ \forall ~ i
\end{equation}

\noindent which turns the matrix inversion into a minimization problem that can be solved, for example, by applying a least-squares methods.
With a perfect response simulation and in the absence of statistical fluctuations, the TopDown algorithm and the fit should yield the same results.
Assuming that the influence of both effects is not too strong, \textsc{Horst} uses both algorithms: 

\noindent First, it solves the problem using the TopDown algorithm. After that, it uses the obtained parameters $s_\mathrm{TopDown}[i]$ as start parameters for a least-squares fit with the boundary condition Eq. (\ref{no_zeros}), setting all negative $s_\mathrm{TopDown}[i]$ to zero.

\section{Computational Aspects}

\subsection{Response Simulations}
\label{sec:response_simulations}

Usually, the response matrix comes from a sequence of simulations with a monoenergetic particle beam using $N_\mathrm{sim}$ primary particles.

\begin{equation}
	\label{response_matrix_normalization}
	r_\mathrm{sim} = r N_\mathrm{sim}
\end{equation}

\noindent In general, different numbers of primary particles may have been used for different simulations, therefore the most general form of $N_\mathrm{sim}$ is a diagonal matrix:

\begin{equation}
	N_\mathrm{sim} = \mathrm{diag}\left( N_\mathrm{sim} [0], N_\mathrm{sim} [1], ..., N_\mathrm{sim} [N] \right)
\end{equation}

\noindent Using the simulated response function, Eq. (\ref{matrix_equation_triangle_explicit}) becomes:

\begin{equation}
	\label{matrix_equation_triangle_simulated}
	\left[ 
		\begin{array}{c}
			\tilde{s}[0] \\
			\tilde{s}[1] \\
			\vdots	\\
			\tilde{s}[N] \\
		\end{array}
	\right]
	= 
	\begin{bmatrix}
		r_\mathrm{sim}[0][0] & r_\mathrm{sim}[0][1] & \hdots & r_\mathrm{sim}[0][N] \\
		0       & r_\mathrm{sim}[1][1] & \hdots & r_\mathrm{sim}[1][N] \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & \hdots & r_\mathrm{sim}[N][N] \\
	\end{bmatrix}
	\cdot
	\left[ 
		\begin{array}{c}
			p[0] \\
			p[1] \\
			\vdots	\\
			p[N] \\
		\end{array}
	\right]
\end{equation}

\noindent In Eq. (\ref{matrix_equation_triangle_simulated}), $p$ is the vector of fit parameters of \textsc{Horst}, which is related to the original spectrum by

\begin{equation}
	\label{fit_parameters}
	s = N_\mathrm{sim} \cdot p
\end{equation}

\subsection{Response Interpolation}

State of the art Monte-Carlo (MC) simulations of the detector response require a lot of computing power to keep the statistical uncertainty of the simulation on a comparable order of magnitude as the statistical uncertainty of the observed spectrum (see also Sec. \ref{Uncertainty}).
Since cross sections often have a smooth dependence on the energy over large energy range, it may not be necessary to perform a simulation for every single incoming-particle energy $E_j$.
Instead, if, for a sufficiently small step in energy,
%
\begin{equation}
\label{weak_energy_dependence}
r[E_i][E_j] \approx r[E_i \pm \Delta E][E_j \pm \Delta E]
\end{equation}
%
\noindent holds, a simulation for a particular energy $E_j$ can be used for a whole energy region $\left[ E_j - \Delta E, E_j + \Delta E \right]$.

\noindent In an earlier version \textsc{Horst} (more precisely: the \textsc{MakeMatrix} program) simply shifted the spectrum of the closest simulated energy to obtain the response for an intermediate energy.
For example, if a simulated existed for $E_N$ but not for $E_{N-1}$ \textsc{Horst} would have modified Eq. (\ref{matrix_equation_triangle_simulated}) in the following way:

\begin{equation}
	\label{matrix_equation_triangle_approximation}
	r_\mathrm{sim} \approx 
	\begin{bmatrix}
		r_\mathrm{sim}[0][0] & r_\mathrm{sim}[0][1] & \hdots & r_\mathrm{sim}[1][N] & r_\mathrm{sim}[0][N]   \\
		0             & r_\mathrm{sim}[1][1] & \hdots & r_\mathrm{sim}[2][N] & r_\mathrm{sim}[1][N]   \\
		\vdots        & \vdots        & \ddots & \vdots        & \vdots          \\
		0             & 0             & \hdots & r_\mathrm{sim}[N][N] & r_\mathrm{sim}[N-1][N] \\
		0             & 0             & \hdots & 0             & r_\mathrm{sim}[N][N]   \\
	\end{bmatrix}
\end{equation}

Simply replacing a column in the spectrum with another column (shift by the appropriate difference in bins) can be seen as a zero-order approximation.
At present, \textsc{Horst} uses a first order approximation and interpolates between the two closest simulated energies.
For example, if $i$ is the bin of interest, and bins $l$ (``low'') and $h$ (``high'') contain the closest simulations, the $i$-th column of the response matrix would be constructed as:

\begin{equation}
	\label{interpolation}
	r[i][j] = \frac{(i-l) r[l][j] + (h-i) r[h][j]}{h-l}
\end{equation}

\noindent For bins below the lowest and above the highest simulated energy, the zero-order approximation is used.

\noindent When implementing the code, it was found to be easier to work with the raw simulated matrix $r_\mathrm{sim}$ instead of $r$.
Therefore, the code needs to be supplied a simulated matrix and a list of the number of primary particles that was used for each simulation.
Using the normalized matrix $r$ directly would complicate the uncertainty calculations, especially when the matrix is rebinned.

\subsection{Uncertainty}
\label{Uncertainty}

The uncertainty of the reconstruction procedure is also taken into account by \textsc{Horst}.
There are three sources of uncertainty in the process described above:

\begin{enumerate}
	\item The statistical uncertainty of the detected spectrum $\Delta \tilde{s}[i]$
	\item The statistical uncertainty of the Monte-Carlo simulation $\Delta r[i][j]$
	\item The systematic uncertainty of the minimization algorithm $\Delta p[i]$
\end{enumerate}

\noindent In principle, number 2 and 3 can be made arbitrarily small by investing more computing power. 
Number 1, on the other hand, is the determined by the measured spectrum.

\noindent \textsc{Horst} outputs all three of them separately for comparison, if executed with the standard options.
In the case of Monte-Carlo uncertainty propagation (see below), the results of the $-u$ and $-U$ option should be compared to see the interplay of 1 and 2.

\noindent The systematic uncertainty of the fit can be obtained from the minimization algorithm in most cases.
\textsc{Horst} uses the estimation given by \textsc{ROOT}.
It is assumed that this estimation can be done with sufficient precision, and the rest of the discussion will focus on the first two sources of uncertainty.

The statistical uncertainties of single bins / matrix elements can be estimated from counting statistics:
%
\begin{equation}
	\label{spectrum_uncertainty}
	\Delta \tilde{s} [i] = \sqrt{ \tilde{s}[i] }
\end{equation}
\begin{equation}
	\label{simulation_uncertainty}
	\Delta r_\mathrm{sim} [i][j] = \sqrt{ r_\mathrm{sim}[i][j] }
\end{equation}

\subsubsection{First-order Propagation of Uncertainty}

The uncertainties can be included approximately using the first-order ``Gaussian'' propagation of uncertainties, which means that they are assumed to be uncorrelated.
By combining Eqs.~\eqref{matrix_equation_triangle_simulated}, \eqref{fit_parameters}, and \eqref{topdown_solution}, $s[i]$ is found to be:

\begin{equation}
	\label{topdown_solution_with_simulation}
	s[i] / N_\mathrm{sim} [i] = p[i] = \frac{\tilde{s}[i] - \sum_{j = i+1}^{N} r_\mathrm{sim}[i][j] p[j]}{r_\mathrm{sim}[i][i]}
\end{equation}

\noindent Starting from the uncertainty of the first parameter, which is either

\begin{equation}
	p[N] = \frac{\Delta \tilde{s} [N]}{N_\mathrm{sim}[N]},
\end{equation}

\noindent for the top-down algorithm, or the output of the least-squares fitting algorithm, the uncertainty of the $i$-th parameter can be obtained from the recursive relation:

\begin{align}
	\Delta p[i] = &\left[ \vphantom{\left(\frac{\Delta \tilde s[i]}{r_\mathrm{sim} [i][i]} \right)^2 } \right. \phantom{+} \left( \frac{\Delta \tilde s[i]}{r_\mathrm{sim} [i][i]} \right)^2 \nonumber \\
	& + \left( \frac{\Delta r_\mathrm{sim} [i][i] \left(\tilde{s}[i] - \sum_{j=i+1}^{N} r_\mathrm{sim} [i][j] p[j]\right)}{r_\mathrm{sim} [i][i]} \right)^2 \\
	& + \sum_{j=i+1}^N \left( \frac{\Delta r_\mathrm{sim} [i][j] p[j]}{r_\mathrm{sim} [i][i]} \right)^2 + \left( \frac{r_\mathrm{sim} [i][j] \Delta p[j]}{r_\mathrm{sim} [i][i]^2} \right)^2 \left. \vphantom{\left(\frac{\Delta \tilde s[i]}{r_\mathrm{sim} [i][i]} \right)^2} \right]^{1/2}. \nonumber
\end{align}

\subsubsection{Monte Carlo Propagation of Uncertainty}

The assumption of independent parameters is cleary an approximation in the case of an unfolding procedure for a typical detector response, because the parameters can be expected to depend on each other.
The most straightforward way to incorporate such correlations is to use a Monte-Carlo (MC) method.
This method assumes that the input parameters (here: $\tilde{s}$, $r$ and $q$) follow a probability distribution which is known or can be estimated.
The final result (here: $s$) is obtained from these input parameters by algebraic manipulations (here: the product of a matrix with a vector).
By sampling many input parameters from their probability distribution and repeating the calculations $N_\mathrm{MC}$ times, a probability distribution for the resulting quantity can be estimated, which contains all the correlations that are introduced by the calculation.

\noindent In \textsc{Horst}, MC propagation of uncertainty is implemented by assuming a Poisson distribution $\mathcal{P}(\mu[i])$ for the content of each bin of the detected spectrum $\tilde{s}[i]$, with the mean value:
%
\begin{equation}
	\label{mc_spectrum_mu}
	\mu [i] \approx \tilde{s}[i]
\end{equation}
%
A Poisson distribution is the typical distribution of results in a counting experiments where every single event occurs independently of the others.
For large values $\mu[i] \geq 10$, it converges to a normal distribution $\mathcal{N}(\mu[i], \sigma[i])$ with the mean value given by Eq. \ref{mc_spectrum_mu} and a standard deviation of:
%
\begin{equation}
	\label{mc_spectrum_sigma}
	\sigma[i] = \approx \sqrt{\tilde{s}[i]}
\end{equation}
%
\textit{(In the case that $\tilde {s} [i] = 0$, $\sigma$ is also set to zero at the moment. However, since the measured spectrum is often very smooth, this might be changed by adapting the uncertainty of surrounding bins.)}

\noindent In each MC iteration, a new detected spectrum and a new response matrix are sampled, assuming the mean value of the distribution is given by Eq. \ref{mc_spectrum_mu}.
In the end, a set of values for each bin $s[i]$ is obtained.
For the complete inclusion of correlations by the MC method, one has to pay the price of a linearly increasing computing time of \textsc{Horst} with the number of iterations.
Before starting an extensive MC sampling procedure, it is always useful to compare the Gaussian uncertainty propagation and the MC method for a few samples to see whether the inclusion of all correlations significantly changes the result.
\end{document}
