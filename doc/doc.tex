\documentclass{article}

\usepackage{amsmath}
\usepackage{hyperref}

\begin{document}

\begin{titlepage}
\Huge
	\textbf{How Horst Works}
\end{titlepage}

\tableofcontents

\newpage

\section{Introduction}
This document describes briefly how the reconstruction of original spectra can be done and how it is implemented in \textbf{Horst}.

\section{Continuous spectrum}
First, treat the particle spectrum $s(E)$ as a continuous function of the energy $E$. 
The detected spectrum will be denoted as $\tilde{s}(E)$.

The content of the detected spectrum at an energy $E$ is related to the original spectrum by a continuous superposition with the detector response function $r(E, E')$:

\begin{equation}
\label{convolution_continuous}
	\tilde{s}(E) = \int r(E, E') \cdot s(E') \mathrm{d}E' 
\end{equation}

This continuous superposition is often also referred to as the convolution with the detector response due to the similarity of both operations, although the mathematical definition of the convolution differs.
The two arguments of $r$ mean that, in principle, any energy of the detected spectrum can be influenced by any energy of the original spectrum.
The diagonal elements $r(E,E)$ are commonly referred to as the detection efficiency, or the full-energy peak (FEP) efficiency.
For a detector with negligible noise and pileup, 

\begin{equation}
	\label{response_condition}
	r(E, E') = 0 ~\mathrm{if}~ E > E'
\end{equation}

i.e. a detector will, at maximum, detect the full energy of a particle, and not more. This is also assumed in the code.

\section{Histograms}
In reality, one almost never measures continuous spectra, but they will be binned in some way.
Denoting all quantities with square brackets instead of braces, and variables with an index in order to indicates discrete values, Eq. (\ref{convolution_continuous}) becomes:

\begin{equation}
	\label{convolution_discrete}
	\tilde{s}[E_i] = \sum_{j = 0}^{N} r[E_i][E_j] \cdot s[E_j]
\end{equation}

The convolution of the response function on the spectrum has now become the action of an $N \times N$ matrix on a vector with $N$ entries:

\begin{align}
	\label{matrix_equation_general}
	\tilde{s} &= r \cdot s = \\
	\label{matrix_equation_explicit}
	\left[ 
		\begin{array}{c}
			\tilde{s}[0] \\
			\tilde{s}[1] \\
			\vdots	\\
			\tilde{s}[N] \\
		\end{array}
	\right]
	&= 
	\begin{bmatrix}
		r[0][0] & r[0][1] & \hdots & r[0][N] \\
		r[1][0] & r[1][1] & \hdots & r[1][N] \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		r[N][0] & r[N][1] & \hdots & r[N][N] \\
	\end{bmatrix}
	\cdot
	\left[ 
		\begin{array}{c}
			s[0] \\
			s[1] \\
			\vdots	\\
			s[N] \\
		\end{array}
	\right]
\end{align}

Each column $r[][i]$ of the the matrix is multiplied by a single entry of the original spectrum. This means that it represents the response of the detector to monoenergetic particles with the energy $E_i$.
Each row $r[i][]$ shows the magnitude of the correlation of this bin with all others.

\section{Solving for the original spectrum}

The idealized problem in Eq. (\ref{matrix_equation_general}) can in principle be solved by inversion of the matrix $r$:

\begin{equation}
	\label{matrix_equation_solved}
	r^{-1} \cdot \tilde{s} = s
\end{equation}

Note that, in a real detection experiment, there is the influence of counting statistics, which causes the bin contents of $\tilde{s}$ to deviate from the true value by a factor $\epsilon$:

\begin{equation}
	\label{matrix_equation_statistics}
	\tilde{s} + \epsilon \approx r \cdot s
\end{equation}

Admitting statistical fluctuations, the equality of the two parts of the equation does not hold any more.
However, equality will be assumed in the following, i.e. it is assumed that statistics are high enough so that the fluctuations do not matter too much.
This would mean that Eq. (\ref{matrix_equation_general}) can still be solved by inversion.

The straightforward solution by inversion can be greatly simplified by applying Eq. (\ref{response_condition}) in discretized form. 
Then, the matrix in Eq. (\ref{matrix_equation_general}) becomes an upper triangular matrix:

\begin{equation}
	\label{matrix_equation_triangle_explicit}
	\left[ 
		\begin{array}{c}
			\tilde{s}[0] \\
			\tilde{s}[1] \\
			\vdots	\\
			\tilde{s}[N] \\
		\end{array}
	\right]
	= 
	\begin{bmatrix}
		r[0][0] & r[0][1] & \hdots & r[0][N] \\
		0       & r[1][1] & \hdots & r[1][N] \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & \hdots & r[N][N] \\
	\end{bmatrix}
	\cdot
	\left[ 
		\begin{array}{c}
			s[0] \\
			s[1] \\
			\vdots	\\
			s[N] \\
		\end{array}
	\right]
\end{equation}

This problem is much easier to solve than with a completely general matrix.
The solution algorithm is based on Gaussian elimination and sometimes called the TopDown algorithm in this context.
It can be visualized as subsequently subtracting the correctly scaled response function of the highest energy bin from the detected spectrum to "peel away" the response and leave only the original spectrum.

Its starting steps are:

\begin{enumerate}
	\item{Starting at the lower right edge of the matrix (highest energy), solve the trivial equation $\tilde{s}[N] = r[N][N] \cdot s[N]$ to scale the full-energy peak of the response function to the detected spectrum. 
		The scaling factor $s[N]=\tilde{s}[N]/r[N][N]$ is the content of bin $N$ in the original spectrum.}
	\item{Multiply the corresponding column $N$ of the response matrix with the factor (the solution for $s[N]$) and subtract this vector on both sides of the equation (to elliminate $s[N]$ from the equation system). In the new, resulting equation system (part of the spectrum still to be unfolded) the highest energy bin of $s$ is now a zero, i.e. the outermost column $r[][N]$ does not matter any more, as well as the lowest row $r[N][]$.}
	\item{Proceed with the equation in row $N-1$ as in the first step, because this one has now become trivial.}
	\item{...}
\end{enumerate}

This algorithm, like all direct solutions of Eq. (\ref{matrix_equation_general}) have the drawback that they can yield unphysical results, i.e. negative bin contents, because the simulated response is not perfect, the statistical effects cumulate, or the energy calibration of the detected spectrum is not perfect.
This can be overcome by imposing the condition

\begin{equation}
	\label{no_zeros}
	\tilde{s}[i] >= 0 ~ \forall ~ i
\end{equation}

which turns the matrix inversion into a minimization problem that can be solved, for example, by applying a least-squares algorithm.
With a perfect response simulation and no statistical fluctuations, the TopDown algorithm and the fit should yield the same results.
Assuming that the influence of both effects is not too strong, \textbf{Horst} uses both algorithms: 

First, it solves the problem using the TopDown algorithm. After that, it uses the obtained parameters $s[i]$ as start parameters for a least-squares fit, setting all negative parameters to zero and imposing Eq. (\ref{no_zeros}).

\section{Response function}
In fact, the response matrix comes from a sequence of simulations with a monoenergetic particle beam using $N_\mathrm{sim}$ primary particles. For simplicity, assume that the same number of primary particles has been used in all simulations, then the simulated response matrix $r_\mathrm{sim}$ is related to $r$ by:
%
\begin{equation}
	r_\mathrm{sim} = N_\mathrm{sim} \cdot r
\end{equation}
%
Using the simulated response function, Eq. (\ref{matrix_equation_triangle_explicit}) becomes:
%
\begin{equation}
	\label{matrix_equation_triangle_simulated}
	\left[ 
		\begin{array}{c}
			\tilde{s}[0] \\
			\tilde{s}[1] \\
			\vdots	\\
			\tilde{s}[N] \\
		\end{array}
	\right]
	= 
	\begin{bmatrix}
		r_{sim}[0][0] & r_{sim}[0][1] & \hdots & r_{sim}[0][N] \\
		0       & r_{sim}[1][1] & \hdots & r_{sim}[1][N] \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & \hdots & r_{sim}[N][N] \\
	\end{bmatrix}
	\cdot
	\left[ 
		\begin{array}{c}
			p[0] \\
			p[1] \\
			\vdots	\\
			p[N] \\
		\end{array}
	\right]
\end{equation}
%
In Eq. (\ref{matrix_equation_triangle_simulated}), $p$ is the vector of fit parameters of \textbf{Horst}, which are related to the original spectrum by
%
\begin{equation}
	s = N_\mathrm{sim} \cdot p
\end{equation}
%
State of the art Monte-Carlo (MC) simulations of the detector response require a lot of computing power because one would like to keep the statistical uncertainty small, which is introduced by them (see also Sec. \ref{Uncertainty}).
They are commonly used for applications where the interaction of particles with matter is of interest, therefore we will restrict the following discussion to MC-simulated response functions and consider that they may have a significant statistical uncertainty.
Often, simulating the detector response for all relevant particle energies $E_j$ would take a lot of time, whereas the relative dependence of the response on the particle energy may be very weak.
If, for a sufficiently small step in energy,
%
\begin{equation}
\label{weak_energy_dependence}
r[E_i][E_j] \approx r[E_i \pm \Delta E][E_j \pm \Delta E]
\end{equation}
%
holds, a simulation for a particular energy $E_j$ can be used for a whole energy region $\left[ E_j - \Delta E, E_j + \Delta E \right]$.
For example if the response of a detector to the energies $E_N$ and $E_{N-1}$ is approximately the same (besides the obvious energy shift of the spectrum), Eq. (\ref{matrix_equation_triangle_simulated}) can be modified by re-using the last row shifted:
%
\begin{equation}
	\label{matrix_equation_triangle_approximation}
	r_{sim} \approx 
	\begin{bmatrix}
		r_{sim}[0][0] & r_{sim}[0][1] & \hdots & r_{sim}[1][N] & r_{sim}[0][N]   \\
		0             & r_{sim}[1][1] & \hdots & r_{sim}[2][N] & r_{sim}[1][N]   \\
		\vdots        & \vdots        & \ddots & \vdots        & \vdots          \\
		%0             & 0             & \hdots & r_{sim}[N-1][N] & r_{sim}[N-2][N] \\
		0             & 0             & \hdots & r_{sim}[N][N] & r_{sim}[N-1][N] \\
		0             & 0             & \hdots & 0             & r_{sim}[N][N]   \\
	\end{bmatrix}
\end{equation}
%
If \textbf{Horst} (actually MakeMatrix) is given a set of simulations whose energy steps are too coarse for the desired resolution, it will apply Eq. (\ref{weak_energy_dependence}) as well and take the closest simulated energy for each bin of the spectrum.
This choice is not done during runtime, but already when the response matrix is built up from simulated responses (by MakeMatrix).

\textit{(Now, one might ask the question: If so many preparatory steps are taken when assembling the response matrix, then why not divide by the factor $N_\mathrm{sim}$ as well to get $s$ directly from the fitting process?
The reason is that one still needs to know the statistical fluctuations $\Delta r_\mathrm{sim}[i][j]$ of the entries of $r_\mathrm{sim}$, which can be calculated as $\sqrt{r_\mathrm{sim}[i][j] }$.).}


\section{Uncertainty}
\label{Uncertainty}

The uncertainty of the reconstruction procedure is also taken into account by \textbf{Horst}.
There are three sources of uncertainty in the process described above:

\begin{enumerate}
	\item The statistical uncertainty of the detected spectrum $\Delta \tilde{s}[i]$
	\item The statistical uncertainty of the Monte-Carlo simulation $\Delta r[i][j]$
	\item The systematic uncertainty of the minimization algorithm $\Delta p[i]$
\end{enumerate}

In principle, number 2 and 3 can be made arbitrarily small at any time by exercising more computing power. 
Number 1, however, is the given by the measured spectrum that one has to work with.

\textbf{Horst} outputs all three of them separately for comparison, if executed with the standard options.
In the case of Monte-Carlo uncertainty propagation, which is described below, the results of the $-u$ and $-U$ option should be compared to see the interplay of 1 and 2.

The systematic uncertainty of the fit can be obtained from the minimization algorithm in most cases.
\textbf{Horst} uses the estimation given by \textbf{ROOT}.
It is assumed that this estimation can be done with sufficient precision, and the rest of the discussion will focus on the first two sources of uncertainty.

The statistical uncertainties of single bins / matrix elements can be estimated from counting statistics:
%
\begin{equation}
	\label{spectrum_uncertainty}
	\Delta \tilde{s} [i] = \sqrt{ s[i] }
\end{equation}
\begin{equation}
	\label{simulation_uncertainty}
	\Delta r [i][j] = \sqrt{ r[i][j] }
\end{equation}
%
In a first approximation, they can be included using Gaussian propagation of uncertainties, which means that they would not be correlated.
Consider first the TopDown algorithm again, which yields a unique result.
In this case, $s[i]$ is given by:
%
% TODO: This seems to be nonsense:
% The solution s[i] is defined by using the solution q[j]=s[j] in a sum with i=j...
% The formula has a false form too: It should be 
% $s[i] = \frac{\tilde{s}[i]}{r_{sim}[i][i]} - \sum_{j > i} \frac{r_{sim}[i][j]}{r_{sim}[i][i]} \cdot s[j]$
% so highly recursive (due to the ellimination algorithm subtracting all previous solutions, which in turn also contain their previous solutions)
% Hence the given uncertainty calculation seems also off ignoring the uncertainties in the previous solutions
%

\begin{equation}
	\label{topdown_solution}
	s[i] = \sum_{j \geq i} r_{sim}[i][j] \cdot q[j] \cdot \tilde{s}[j]
\end{equation}
%
In Eq. (\ref{topdown_solution}), the parameters $q[j]$ are determined by the solution of Eq. (\ref{matrix_equation_triangle_explicit}) and assumed to be exact.
Then, the uncertainty $\Delta s[i]$ is given by
%
\begin{equation}
	\label{topdown_uncertainty}
	\Delta s[i] = \sqrt{\sum_{j \geq i} \left( \Delta r_{sim}[i][j] \cdot q[j] \cdot \tilde{s}[j] \right)^2 + \sum_{j \geq i} \left( r_{sim}[i][j] \cdot q[j] \cdot \Delta \tilde{s}[j] \right)^2}
\end{equation}
%
using $\Delta r[i][j]$ and $\Delta \tilde{s} [i]$ from Eq. (\ref{simulation_uncertainty}) and (\ref{spectrum_uncertainty}).
If the uncertainty of the parameters $q$ are known, they can also be included by introducing $\Delta q[i]$, which creates a new term in the square-root of Eq. (\ref{topdown_uncertainty}).
This is used by \textbf{Horst} as an approximation of the uncertainty of the reconstruction procedure.

However, if the parameters are strongly correlated, which is easily possible if the matrix elements are of similar magnitude, then this approach may be false in either direction, depending on whether the correlations are destructive or constructive.
The most straightforward way to incorporate such correlations is to use Monte-Carlo propagation of uncertainty.
This method assumes that the input parameters (here: $\tilde{s}$, $r$ and $q$) follow a probability distribution which is known or can be estimated.
The final result (here: $s$) is obtained from these input parameters by algebraic manipulations (here: the product of a matrix with a vector).
By sampling many input parameters from their probability distribution and repeating the algebra over and over again, a probability distribution for the resulting quantity can be obtained, which contains all the correlations that are introduced by the calculation.

\textbf{Horst} can also employ this method to get an accurate estimate of the uncertainty of the reconstructed spectrum.
It is implemented by assuming a Poisson distribution $\mathcal{P}(\mu[i])$ for the content of each bin of the detected spectrum $\tilde{s}[i]$, with the mean value:
%
\begin{equation}
	\label{mc_spectrum_mu}
	\mu [i] \approx \tilde{s}[i]
\end{equation}
%
A Poisson distribution is the typical distribution of results in a counting experiments where every single event occurs independently of the others.
For large values $\mu[i] \geq 10$, it converges to a normal distribution $\mathcal{N}(\mu[i], \sigma[i])$ with the mean value given by Eq. \ref{mc_spectrum_mu} and a standard deviation of:
%
\begin{equation}
	\label{mc_spectrum_sigma}
	\sigma[i] = \approx \sqrt{\tilde{s}[i]}
\end{equation}
%
\textit{(In the case that $\tilde {s} [i] = 0$, $\sigma$ is also set to zero at the moment. However, since the measured spectrum is often very smooth, this might be changed by adapting the uncertainty of surrounding bins.)}

In each Monte-Carlo iteration, a new detected spectrum and a new response matrix are sampled, assuming the mean value of the distribution is given by Eq. \ref{mc_spectrum_mu}.
In the end, a set of values for each bin $s[i]$ is obtained.
\textit{Horst} will return the mean value and the standard deviation of this set.
For the complete inclusion of correlations by the Monte-Carlo method, one has to pay the price of a linearly increasing computing time of \textit{Horst} with the number of iterations.
If several similar calculations need to be done, it is always useful to compare the Gaussian uncertainty propagation and the Monte-Carlo method to see whether the improvement is actually significant.
\end{document}
